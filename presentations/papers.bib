@incollection{eu_law_citations,
	title = {Giving {Every} {Case} {Its} ({Legal}) {Due} - {The} {Contribution} of {Citation} {Networks} and {Text} {Similarity} {Techniques} to {Legal} {Studies} of {European} {Union} {Law}},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-838-9-59},
	urldate = {2024-09-13},
	booktitle = {Legal {Knowledge} and {Information} {Systems}},
	publisher = {IOS Press},
	author = {Panagis, Yannis and Šadl, Urška and Tarissan, Fabien},
	year = {2017},
	doi = {10.3233/978-1-61499-838-9-59},
	pages = {59--68},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/29ZW5KXY/Panagis et al. - 2017 - Giving Every Case Its (Legal) Due - The Contribution of Citation Networks and Text Similarity Techni.pdf:application/pdf},
}

@article{structure_us_federal_law,
	title = {The {Structure} and {Dynamics} of {Modern} {United} {States} {Federal} {Case} {Law}},
	volume = {9},
	issn = {2296-424X},
	url = {https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2021.695219/full},
	doi = {10.3389/fphy.2021.695219},
	abstract = {{\textless}p{\textgreater}The structure and dynamics of modern United States Federal Case Law are examined here. The analyses utilize large-scale network analysis tools, natural language processing techniques, and information theory to examine all the federal opinions in the Court Listener database, containing approximately 1.3 million judicial opinions and 11.4 million citations. The analyses are focused on modern United States Federal Case Law, as cases in the Court Listener database range from approximately 1926–2020 and include most Federal jurisdictions. We examine the data set from a structural perspective using the citation network, overall and by time and space (jurisdiction). In addition to citation structure, we examine the dataset from a topical and information theoretic perspective, again, overall and by time and space.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-09-13},
	journal = {Frontiers in Physics},
	author = {Adusumilli, Keerthi and Brown, Bradford and Harrison, Joey and Koehler, Matthew and Kutarnia, Jason and Michel, Shaun and Olivier, Max and Pfeifer, Craig and Slater, Zoryanna and Thompson, William and Vetter, Dianna and Zacharowicz, Renee},
	month = jan,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {Citation networks, Information Theory, Judicial opinions, Law, topic models},
	file = {Full Text:/home/rambip/Documents/Zotero/storage/LV97UUM8/Adusumilli et al. - 2022 - The Structure and Dynamics of Modern United States Federal Case Law.pdf:application/pdf},
}

@misc{renumbering_tax,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {{AI} for {Tax} {Analogies} and {Code} {Renumbering}},
	url = {https://papers.ssrn.com/abstract=3850796},
	abstract = {We present an artificial intelligence tool that can complete analogies in tax law and provide evidence-based guidance on how Congress can renumber IRC sections in future tax reform efforts.},
	language = {en},
	urldate = {2024-09-13},
	author = {Blair-Stanek, Andrew and Van Durme, Benjamin},
	month = may,
	year = {2021},
	keywords = {tax, tax Code, tax law, tax reform, vector representation, word2vec},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/SB2SBUTZ/Blair-Stanek and Van Durme - 2021 - AI for Tax Analogies and Code Renumbering.pdf:application/pdf},
}


@article{ljp_graphs,
	title = {Legal {Judgment} {Prediction} via {Heterogeneous} {Graphs} and {Knowledge} of {Law} {Articles}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/5/2531},
	doi = {10.3390/app12052531},
	abstract = {Legal judgment prediction (LJP) is a crucial task in legal intelligence to predict charges, law articles and terms of penalties based on case fact description texts. Although existing methods perform well, they still have many shortcomings. First, the existing methods have significant limitations in understanding long documents, especially those based on RNNs and BERT. Secondly, the existing methods are not good at solving the problem of similar charges and do not fully and effectively integrate the information of law articles. To address the above problems, we propose a novel LJP method. Firstly, we improve the model’s comprehension of the whole document based on a graph neural network approach. Then, we design a graph attention network-based law article distinction extractor to distinguish similar law articles. Finally, we design a graph fusion method to fuse heterogeneous graphs of text and external knowledge (law article group distinction information). The experiments show that the method could effectively improve LJP performance. The experimental metrics are superior to the existing state of the art.},
	language = {en},
	number = {5},
	urldate = {2024-09-22},
	journal = {Applied Sciences},
	author = {Zhao, Qihui and Gao, Tianhan and Zhou, Song and Li, Dapeng and Wen, Yingyou},
	month = jan,
	year = {2022},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {graph attention network, graph convolutional network, graph fusion method, heterogeneous graphs, legal judgment prediction},
	pages = {2531},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/BXD34UXF/Zhao et al. - 2022 - Legal Judgment Prediction via Heterogeneous Graphs and Knowledge of Law Articles.pdf:application/pdf},
}

@article{growth_law,
	title = {Complex {Societies} and the {Growth} of the {Law}},
	volume = {10},
	issn = {2045-2322},
	url = {http://arxiv.org/abs/2005.07646},
	doi = {10.1038/s41598-020-73623-x},
	abstract = {While a large number of informal factors inﬂuence how people interact, modern societies rely upon law as a primary mechanism to formally control human behaviour. How legal rules impact societal development depends on the interplay between two types of actors: the people who create the rules and the people to which the rules potentially apply. We hypothesise that an increasingly diverse and interconnected society might create increasingly diverse and interconnected rules, and assert that legal networks provide a useful lens through which to observe the interaction between law and society. To evaluate these propositions, we present a novel and generalizable model of statutory materials as multidimensional, time-evolving document networks. Applying this model to the federal legislation of the United States and Germany, we ﬁnd impressive expansion in the size and complexity of laws over the past two and a half decades. We investigate the sources of this development using methods from network science and natural language processing. To allow for cross-country comparisons over time, we algorithmically reorganise the legislative materials of the United States and Germany into cluster families that reﬂect legal topics. This reorganisation reveals that the main driver behind the growth of the law in both jurisdictions is the expansion of the welfare state, backed by an expansion of the tax state.},
	language = {en},
	number = {1},
	urldate = {2024-09-26},
	journal = {Scientific Reports},
	author = {Katz, Daniel Martin and Coupette, Corinna and Beckedorf, Janis and Hartung, Dirk},
	month = oct,
	year = {2020},
	note = {arXiv:2005.07646 [physics]},
	keywords = {Computer Science - Social and Information Networks, Physics - Physics and Society, non-technical},
	pages = {18737},
	file = {PDF:/home/rambip/Documents/Zotero/storage/K2PKFKIW/Katz et al. - 2020 - Complex Societies and the Growth of the Law.pdf:application/pdf},
}



@misc{network_french_codes_1,
	title = {A {Network} {Approach} to the {French} {System} of {Legal} codes - {Part} {I}: {Analysis} of a {Dense} {Network}},
	shorttitle = {A {Network} {Approach} to the {French} {System} of {Legal} codes - {Part} {I}},
	url = {http://arxiv.org/abs/1201.1262},
	doi = {10.48550/arXiv.1201.1262},
	abstract = {We explore one aspect of the structure of a codified legal system at the national level using a new type of representation to understand the strong or weak dependencies between the various fields of law. In Part I of this study, we analyze the graph associated with the network in which each French legal code is a vertex and an edge is produced between two vertices when a code cites another code at least one time. We show that this network distinguishes from many other real networks from a high density, giving it a particular structure that we call concentrated world and that differentiates a national legal system (as considered with a resolution at the code level) from small-world graphs identified in many social networks. Our analysis then shows that a few communities (groups of highly wired vertices) of codes covering large domains of regulation are structuring the whole system. Indeed we mainly find a central group of influent codes, a group of codes related to social issues and a group of codes dealing with territories and natural resources. The study of this codified legal system is also of interest in the field of the analysis of real networks. In particular we examine the impact of the high density on the structural characteristics of the graph and on the ways communities are searched for. Finally we provide an original visualization of this graph on an hemicyle-like plot, this representation being based on a statistical reduction of dissimilarity measures between vertices. In Part II (a following paper) we show how the consideration of the weights attributed to each edge in the network in proportion to the number of citations between two vertices (codes) allows deepening the analysis of the French legal system.},
	urldate = {2024-10-01},
	publisher = {arXiv},
	author = {Boulet, Romain},
	month = nov,
	year = {2011},
	note = {arXiv:1201.1262 [physics]},
	keywords = {Computer Science - Social and Information Networks, Physics - Physics and Society},
	file = {arXiv Fulltext PDF:/home/rambip/Documents/Zotero/storage/CW5DA8GE/Boulet - 2011 - A Network Approach to the French System of Legal codes - Part I Analysis of a Dense Network.pdf:application/pdf;arXiv.org Snapshot:/home/rambip/Documents/Zotero/storage/QXGQMI5R/1201.html:text/html},
}

@article{network_french_codes_2,
	title = {Network approach to the {French} system of legal codes part {II}: the role of the weights in a network},
	volume = {26},
	issn = {1572-8382},
	shorttitle = {Network approach to the {French} system of legal codes part {II}},
	url = {https://doi.org/10.1007/s10506-017-9204-y},
	doi = {10.1007/s10506-017-9204-y},
	abstract = {Unlike usual real graphs which have a low number of edges, we study here a dense network constructed from legal citations. This study is achieved on the simple graph and on the multiple graph associated to this legal network, this allows exploring the behavior of the network structural properties and communities by considering the weighted graph and see which additional information are provided by the weights. We propose new measures to assess the role of the weights in the network structure and to appreciate the weights repartition. Then we compare the communities obtained on the simple graph and on the weighted graph. We also extend to weighted networks the amphitheater-like representation (exposed in a previous work) of this legal network. Finally we evaluate the robustness of our measures and methods thus taking into account potential errors which may occur by getting data or building the network. Our methodology may open new perspectives in the analysis of weighted networks.},
	language = {en},
	number = {1},
	urldate = {2024-10-01},
	journal = {Artificial Intelligence and Law},
	author = {Boulet, Romain and Mazzega, Pierre and Bourcier, Danièle},
	month = mar,
	year = {2018},
	keywords = {Artificial Intelligence, Codified legal system, Communities, Graph, Legal code, Structural measures, Weighted network},
	pages = {23--47},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/4WSDN933/Boulet et al. - 2018 - Network approach to the French system of legal codes part II the role of the weights in a network.pdf:application/pdf},
}

@article{degree_detail_law,
	title = {On the desirable degree of detail in the law},
	volume = {2},
	issn = {1572-9990},
	url = {https://doi.org/10.1007/BF01540806},
	doi = {10.1007/BF01540806},
	abstract = {The desirable degree of detail in the law has not been previously discussed. The point of this article is to begin the discussion by raising a number of problems. The first deals with the fact that a highly detailed law cannot, of course, be remembered and, in fact, may be very hard even to discover. Second, if the law is not highly detailed, it is apt to be uncertain in marginal cases, of which there should be many. Detail can be added to the law either by judicial decision or legislation or by some kind of special body as in France. In any event, however, there will certainly be cases in which it is not clear what the law is and there will be at least some obscurities in the law. These problems are discussed and not solved in this paper. It is intended to start the discussion, not finish it.},
	language = {en},
	number = {3},
	urldate = {2024-10-02},
	journal = {European Journal of Law and Economics},
	author = {Tullock, Gordon},
	month = sep,
	year = {1995},
	keywords = {law, detail, policy, non-technical},
	pages = {199--209},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/MJ5KDAFA/Tullock - 1995 - On the desirable degree of detail in the law.pdf:application/pdf},
}


@misc{topic_modelling_legal_bert,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Topic {Modelling} of {Legal} {Documents} via {LEGAL}-{BERT}},
	url = {https://papers.ssrn.com/abstract=4539091},
	doi = {10.2139/ssrn.4539091},
	abstract = {Legal text processing is a challenging task for modeling approaches due to the peculiarities inherent to its features, such as long texts and their technical vocabulary. Topic modeling consists of discovering a semantic structure in the text. This way, it requires specific approaches. The relevant topics strongly depend on the context in which the legal documents will be presented. This work aims to describe and evaluate the use of BERTopic for topic modeling in legal documents. The authors have focused on a subset of landmark cases from the US Caselaw dataset to evaluate the impact of topic modeling, via domain-specific embeddings pre-trained from LEGAL-BERT. The research investigated different variations of generating sentence embeddings from the cases. Results here presented demonstrate that considering the references to statutory law (e.g. US Code) during the process of text embeddings improves the quality of topic modeling.},
	language = {en},
	urldate = {2024-10-20},
	publisher = {Social Science Research Network},
	author = {Silveira, Raquel and Fernandes, Carlos Gustavo and Araujo Monteiro Neto, Joao and Furtado, Vasco and Pimentel Filho, J. Ernesto},
	month = jun,
	year = {2021},
	keywords = {American Case Law, Contextualized Embeddings, Natural Language Processing (NLP)},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/4KYBHKUG/Silveira et al. - 2021 - Topic Modelling of Legal Documents via LEGAL-BERT.pdf:application/pdf},
}

@inproceedings{eu_law_classification,
	title = {Graph-{Based} {Approach} for {European} {Law} {Classification}},
	url = {https://ieeexplore.ieee.org/document/10386684},
	doi = {10.1109/BigData59044.2023.10386684},
	abstract = {Deep learning, owing to its transformative influence across a myriad of sectors, has recently made its foray into the legal domain, instigated by the surge in digitization. Among the multitude of applications in this space, legal document classification emerges as a pivotal yet complex undertaking. Legal texts, characterized by unique domain-centric semantics and intricate linguistic patterns, necessitate precision-driven classification systems for numerous practical implications. This paper illuminates the challenges and opportunities in automating the classification of European Union (EU) legal documents, emphasizing the interrelationships among statutes and the hierarchical nature of legal references. In this context, we introduce a novel graph data modeling technique that adeptly marries content-centric indicators with the relational dynamics inherent among diverse legal documents. Central to our approach is a framework that melds text embeddings with graph neural networks for the classification of legal documents aligned with their subject-based directories. Empirical evaluations on the EU law dataset underline the efficacy of our model across varying granularities, from general thematic categories to intricate subtopics. This endeavor not only augments the comprehensibility and accessibility of EU jurisprudence but also holds significant implications across regulatory compliance, legal research, and policy formulation, underscoring the potential of deep learning in reshaping legal paradigms.},
	urldate = {2024-10-20},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Russo, Raffaele and Giuseppe, Giuliano Di and Vanacore, Alessandro and Gatta, Valerio La and Ferraro, Antonino and Galli, Antonio and Postiglione, Marco and Moscato, Vincenzo},
	month = dec,
	year = {2023},
	keywords = {Law, Big Data, Deep learning, Europe, graph neural networks, Graph neural networks, legal document classification, Linguistics, natural language processing, Semantics},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/home/rambip/Documents/Zotero/storage/AF2HY26W/10386684.html:text/html},
}


@article{simplification_qualite_droit,
	title = {Simplification et qualité du droit},
	language = {fr},
	author = {Cassin, René and Sauvé, Jean-Marc and Stirn, Bernard and Toutée, Henri and Gérard, Patrick},
	keywords = {non-technical},
	file = {PDF:/home/rambip/Documents/Zotero/storage/A6AKGDLL/Cassin et al. - (ancienne collection Études et Documents du Conseil d’État).pdf:application/pdf;Snapshot:/home/rambip/Documents/Zotero/storage/7L33R6F7/simplification-et-qualite-du-droit.html:text/html},
}

@article{knoledge_graphs_simplify_text,
	title = {Can {Knowledge} {Graphs} {Simplify} {Text}?},
	url = {https://dl.acm.org/doi/10.1145/3583780.3615514},
	doi = {10.1145/3583780.3615514},
	abstract = {Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.},
	language = {en},
	urldate = {2024-10-29},
	journal = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
	author = {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang and Wang, Daisy Zhe},
	month = oct,
	year = {2023},
	note = {Conference Name: CIKM '23: The 32nd ACM International Conference on Information and Knowledge Management
ISBN: 9798400701245
Place: Birmingham United Kingdom
Publisher: ACM},
	pages = {379--389},
	file = {Full Text:/home/rambip/Documents/Zotero/storage/KDRDBCLC/Colas et al. - 2023 - Can Knowledge Graphs Simplify Text.pdf:application/pdf},
}

@article{llms4ol,
	title = {{LLMs4OL}: {Large} {Language} {Models} for {Ontology} {Learning}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	shorttitle = {{LLMs4OL}},
	url = {https://arxiv.org/abs/2307.16648},
	doi = {10.48550/ARXIV.2307.16648},
	abstract = {We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: {\textbackslash}textit\{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?\} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.},
	urldate = {2024-11-20},
	author = {Giglou, Hamed Babaei and D'Souza, Jennifer and Auer, Sören},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Information Theory (cs.IT), Machine Learning (cs.LG)},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/9UI4E79B/Giglou et al. - 2023 - LLMs4OL Large Language Models for Ontology Learning.pdf:application/pdf},
}

@incollection{classifying_legal_norms,
	title = {Classifying {Legal} {Norms} with {Active} {Machine} {Learning}},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-838-9-11},
	urldate = {2024-11-27},
	booktitle = {Legal {Knowledge} and {Information} {Systems}},
	publisher = {IOS Press},
	author = {Waltl, Bernhard and Muhr, Johannes and Glaser, Ingo and Bonczek, Georg and Scepankova, Elena and Matthes, Florian},
	year = {2017},
	doi = {10.3233/978-1-61499-838-9-11},
	pages = {11--20},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/H2A9GAGU/Waltl et al. - 2017 - Classifying Legal Norms with Active Machine Learning.pdf:application/pdf},
}

@article{embeddings_for_annotation,
	title = {Sentence {Embeddings} and {High}-{Speed} {Similarity} {Search} for {Fast} {Computer} {Assisted} {Annotation} of {Legal} {Documents}},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://ebooks.iospress.nl/doi/10.3233/FAIA200860},
	doi = {10.3233/FAIA200860},
	abstract = {Human-performed annotation of sentences in legal documents is an important prerequisite to many machine learning based systems supporting legal tasks. Typically, the annotation is done sequentially, sentence by sentence, which is often time consuming and, hence, expensive. In this paper, we introduce a proof-of-concept system for annotating sentences “laterally.” The approach is based on the observation that sentences that are similar in meaning often have the same label in terms of a particular type system. We use this observation in allowing annotators to quickly view and annotate sentences that are semantically similar to a given sentence, across an entire corpus of documents. Here, we present the interface of the system and empirically evaluate the approach. The experiments show that lateral annotation has the potential to make the annotation process quicker and more consistent.},
	urldate = {2024-11-30},
	author = {Westermann, Hannes and Šavelka, Jaromír and Walker, Vern R. and Ashley, Kevin D. and Benyekhlef, Karim},
	editor = {Villata, Serena and Harašta, Jakub and Křemen, Petr},
	month = dec,
	year = {2020},
	doi = {10.3233/FAIA200860},
	note = {Book Title: Frontiers in Artificial Intelligence and Applications
ISBN: 9781643681504 9781643681511
Publisher: IOS Press},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/T7MSNVWW/Westermann et al. - 2020 - Sentence Embeddings and High-Speed Similarity Search for Fast Computer Assisted Annotation of Legal.pdf:application/pdf},
}

@article{zero_shot_gpt_annotation,
	title = {Unlocking {Practical} {Applications} in {Legal} {Domain}: {Evaluation} of {GPT} for {Zero}-{Shot} {Semantic} {Annotation} of {Legal} {Texts}},
	shorttitle = {Unlocking {Practical} {Applications} in {Legal} {Domain}},
	url = {https://dl.acm.org/doi/10.1145/3594536.3595161},
	doi = {10.1145/3594536.3595161},
	abstract = {We evaluated the capability of a state-of-the-art generative pretrained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F1 = .73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.},
	language = {en},
	urldate = {2024-11-30},
	journal = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
	author = {Savelka, Jaromir},
	month = jun,
	year = {2023},
	note = {Conference Name: ICAIL 2023: Nineteenth International Conference on Artificial Intelligence and Law
ISBN: 9798400701979
Place: Braga Portugal
Publisher: ACM},
	pages = {447--451},
	file = {Full Text:/home/rambip/Documents/Zotero/storage/GBX9SCMS/Savelka - 2023 - Unlocking Practical Applications in Legal Domain Evaluation of GPT for Zero-Shot Semantic Annotatio.pdf:application/pdf},
}


@article{construction_legal_kb_llm,
	title = {Construction of {Legal} {Knowledge} {Graph} {Based} on {Knowledge}-{Enhanced} {Large} {Language} {Models}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/15/11/666},
	doi = {10.3390/info15110666},
	abstract = {Legal knowledge involves multidimensional heterogeneous knowledge such as legal provisions, judicial interpretations, judicial cases, and defenses, which requires extremely high relevance and accuracy of knowledge. Meanwhile, the construction of a legal knowledge reasoning system also faces challenges in obtaining, processing, and sharing multisource heterogeneous knowledge. The knowledge graph technology, which is a knowledge organization form with triples as the basic unit, is able to efficiently transform multisource heterogeneous information into a knowledge representation form close to human cognition. Taking the automated construction of the Chinese legal knowledge graph (CLKG) as a case scenario, this paper presents a joint knowledge enhancement model (JKEM), where prior knowledge is embedded into a large language model (LLM), and the LLM is fine-tuned through the prefix of the prior knowledge data. Under the condition of freezing most parameters of the LLM, this fine-tuning scheme adds continuous deep prompts as prefix tokens to the input sequences of different layers, which can significantly improve the accuracy of knowledge extraction. The results show that the knowledge extraction accuracy of the JKEM in this paper reaches 90.92\%. Based on the superior performance of this model, the CLKG is further constructed, which contains 3480 knowledge triples composed of 9 entities and 2 relationships, providing strong support for an in-depth understanding of the complex relationships in the legal field.},
	language = {en},
	number = {11},
	urldate = {2024-12-08},
	journal = {Information},
	author = {Li, Jun and Qian, Lu and Liu, Peifeng and Liu, Taoxiong},
	month = nov,
	year = {2024},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {domain knowledge graph, knowledge engineering, knowledge extraction, large language model, legal knowledge},
	pages = {666},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/CB4ERJHZ/Li et al. - 2024 - Construction of Legal Knowledge Graph Based on Knowledge-Enhanced Large Language Models.pdf:application/pdf},
}



@article{taking_stock,
	title = {Taking stock of legal ontologies: a feature-based comparative analysis},
	volume = {28},
	issn = {0924-8463, 1572-8382},
	shorttitle = {Taking stock of legal ontologies},
	url = {http://link.springer.com/10.1007/s10506-019-09252-1},
	doi = {10.1007/s10506-019-09252-1},
	abstract = {Ontologies represent the standard way to model the knowledge about speciﬁc domains. This holds also for the legal domain where several ontologies have been put forward to model speciﬁc kinds of legal knowledge. Both for standard users and for law scholars, it is often di cult to have an overall view on the existing alternatives, their main features and their interlinking with the other ontologies. To answer this need, in this paper, we address an analysis of the state-of-the-art in legal ontologies and we characterise them along with some distinctive features. This paper aims to guide generic users and law experts in selecting the legal ontology that better ﬁts their needs and in understanding its speciﬁcity so that proper extensions to the selected model could be investigated.},
	language = {en},
	number = {2},
	urldate = {2024-12-08},
	journal = {Artificial Intelligence and Law},
	author = {Leone, Valentina and Di Caro, Luigi and Villata, Serena},
	month = jun,
	year = {2020},
	pages = {207--235},
	file = {PDF:/home/rambip/Documents/Zotero/storage/JA5PG29J/Leone et al. - 2020 - Taking stock of legal ontologies a feature-based comparative analysis.pdf:application/pdf},
}

@inproceedings{functionnal_ontology,
	title = {A functional ontology of law},
	url = {https://www.semanticscholar.org/paper/A-functional-ontology-of-law-Valente-Breuker/f9102f2c3749cf5a08367b6073133f1e4981b264},
	abstract = {Ontological assumptions are at the very hart of the enterprise of representing knowledge. In AI \& Law, these assumptions re(cid:13)ect an underlying view of what law is made of, what legal knowledge is, which knowledge categories play a role in Law and how they interrelate. By and large, however, ontological issues have been rather neglected in AI \& Law. The most common ontological view on Law is very simple, and divides legal knowledge into two orthogonal types: rules and cases. In this paper, we will argue extensively in favor of an explicit attention to ontological issues in AI \& Law, and put forward this view by proposing a functional ontology of Law. This ontology is basically a set of interconnected primitive categories and sub-categories of legal knowledge proposed under a teleological and functional view about the legal system. We will discuss the proposed categories and their interrelations, as well as some of the implications of its use in legal knowledge engineering.},
	urldate = {2024-12-09},
	author = {Valente, A. and Breuker, J.},
	year = {1994},
	keywords = {non-technical},
	file = {A_Functional_Ontology_of_Law:/home/rambip/Documents/Zotero/storage/AFG26WVN/A_Functional_Ontology_of_Law.pdf:application/pdf},
}

@article{aske,
	title = {Enforcing legal information extraction through context-aware techniques: {The} {ASKE} approach},
	volume = {52},
	issn = {0267-3649},
	shorttitle = {Enforcing legal information extraction through context-aware techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S0267364923001139},
	doi = {10.1016/j.clsr.2023.105903},
	abstract = {To cope with the growing volume, complexity, and articulation of legal documents as well as to foster digital justice and digital law, increasing effort is being devoted to legal knowledge extraction and digital transformation processes. In this paper, we present the ASKE (Automated System for Knowledge Extraction) approach to legal knowledge extraction, based on a combination of context-aware embedding models and zero-shot learning techniques into a three-phase extraction cycle, which is executed a number of times (called generations) to progressively extract concepts representative of the different meanings of terminology used in legal documents chunks. A graph-based data structure called ASKE Conceptual Graph is initially populated through a data preparation step, and it is continuously enriched at each ASKE generation with results of document chunk classification, new extracted terminology, and newly derived concepts. A quantitative evaluation of ASKE knowledge extraction and document classification is provided by considering the EurLex dataset. Furthermore, we present the results of applying ASKE to a real case-study of Italian case law decisions with qualitative feedback from legal experts in the framework of an ongoing national research project.},
	urldate = {2024-12-11},
	journal = {Computer Law \& Security Review},
	author = {Castano, Silvana and Ferrara, Alfio and Furiosi, Emanuela and Montanelli, Stefano and Picascia, Sergio and Riva, Davide and Stefanetti, Carolina},
	month = apr,
	year = {2024},
	keywords = {Digital justice, Legal knowledge extraction, Legal knowledge graph, Natural Language Processing},
	pages = {105903},
	file = {ScienceDirect Snapshot:/home/rambip/Documents/Zotero/storage/GDBA52KB/S0267364923001139.html:text/html},
}

@misc{genie,
	title = {{GenIE}: {Generative} {Information} {Extraction}},
	shorttitle = {{GenIE}},
	url = {http://arxiv.org/abs/2112.08340},
	doi = {10.48550/arXiv.2112.08340},
	abstract = {Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction. Code, data and models available at https://github.com/epfl-dlab/GenIE.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Josifoski, Martin and Cao, Nicola De and Peyrard, Maxime and Petroni, Fabio and West, Robert},
	month = apr,
	year = {2022},
	note = {arXiv:2112.08340 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/rambip/Documents/Zotero/storage/23YYURX4/Josifoski et al. - 2022 - GenIE Generative Information Extraction.pdf:application/pdf;Snapshot:/home/rambip/Documents/Zotero/storage/TCZSJPIM/2112.html:text/html},
}

@article{instructuie,
	title = {{InstructUIE}: {Multi}-task {Instruction} {Tuning} for {Unified} {Information} {Extraction}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{InstructUIE}},
	url = {https://arxiv.org/abs/2304.08085},
	doi = {10.48550/ARXIV.2304.08085},
	abstract = {Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.},
	urldate = {2024-12-18},
	author = {Wang, Xiao and Zhou, Weikang and Zu, Can and Xia, Han and Chen, Tianze and Zhang, Yuansen and Zheng, Rui and Ye, Junjie and Zhang, Qi and Gui, Tao and Kang, Jihua and Yang, Jingsheng and Li, Siyuan and Du, Chunsai},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/KE75NNL5/Wang et al. - 2023 - InstructUIE Multi-task Instruction Tuning for Unified Information Extraction.pdf:application/pdf},
}

@inproceedings{nell, Title = {Never-Ending Learning}, Author = {T. Mitchell and W. Cohen and E. Hruschka and P. Talukdar and J. Betteridge and A. Carlson and B. Dalvi and M. Gardner and B. Kisiel and J. Krishnamurthy and N. Lao and K. Mazaitis and T. Mohamed and N. Nakashole and E. Platanios and A. Ritter and M. Samadi and B. Settles and R. Wang and D. Wijaya and A. Gupta and X. Chen and A. Saparov and M. Greaves and J. Welling}, Booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)}, Year = {2015}} 

@misc{openie,
    title={Towards Generalized Open Information Extraction}, 
    author={Bowen Yu and Zhenyu Zhang and Jingyang Li and Haiyang Yu and Tingwen Liu and Jian Sun and Yongbin Li and Bin Wang},
    year={2022},
    eprint={2211.15987},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2211.15987}, 
}

@article{concept_of_law,
    author = {Noonan, John T., Jr.},
    title = {THE CONCEPT OF LAW. By H. L. A. Hart. Oxford: Oxford University Press, 1961. Pp. viii, 263. 21s.},
    journal = {The American Journal of Jurisprudence},
    volume = {7},
    number = {1},
    pages = {169-177},
    year = {1962},
    month = {06},
    issn = {0065-8995},
    doi = {10.1093/ajj/7.1.169},
    url = {https://doi.org/10.1093/ajj/7.1.169},
    eprint = {https://academic.oup.com/ajj/article-pdf/7/1/169/6654709/ajj-7-169.pdf},
}

@misc{saulm,
    title={SaulLM-7B: A pioneering Large Language Model for Law}, 
    author={Pierre Colombo and Telmo Pessoa Pires and Malik Boudiaf and Dominic Culver and Rui Melo and Caio Corro and Andre F. T. Martins and Fabrizio Esposito and Vera Lúcia Raposo and Sofia Morgado and Michael Desa},
    year={2024},
    eprint={2403.03883},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2403.03883}, 
}

@InProceedings{flaubert,
    author    = {Le, Hang  and  Vial, Lo\"{i}c  and  Frej, Jibril  and  Segonne, Vincent  and  Coavoux, Maximin  and  Lecouteux, Benjamin  and  Allauzen, Alexandre  and  Crabb\'{e}, Beno\^{i}t  and  Besacier, Laurent  and  Schwab, Didier},
    title     = {FlauBERT: Unsupervised Language Model Pre-training for French},
    booktitle = {Proceedings of The 12th Language Resources and Evaluation Conference},
    month     = {May},
    year      = {2020},
    address   = {Marseille, France},
    publisher = {European Language Resources Association},
    pages     = {2479--2490},
    url       = {https://www.aclweb.org/anthology/2020.lrec-1.302}
}

@misc{camembert,
      title={CamemBERT 2.0: A Smarter French Language Model Aged to Perfection}, 
      author={Wissam Antoun and Francis Kulumba and Rian Touchent and Éric de la Clergerie and Benoît Sagot and Djamé Seddah},
      year={2024},
      eprint={2411.08868},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.08868}, 
}

@misc{croissantllm,
      title={CroissantLLM: A Truly Bilingual French-English Language Model}, 
      author={Manuel Faysse and Patrick Fernandes and Nuno M. Guerreiro and António Loison and Duarte M. Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro H. Martins and Antoni Bigata Casademunt and François Yvon and André F. T. Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
      year={2024},
      eprint={2402.00786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00786}, 
}

@article{scaling_monosemanticity,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}


@inproceedings{meaning_representations,
    title = "A Survey of Meaning Representations {--} From Theory to Practical Utility",
    author = "Sadeddine, Zacchary  and
      Opitz, Juri  and
      Suchanek, Fabian",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.159/",
    doi = "10.18653/v1/2024.naacl-long.159",
    pages = "2877--2892",
    abstract = "Symbolic meaning representations of natural language text have been studied since at least the 1960s. With the availability of large annotated corpora, and more powerful machine learning tools, the field has recently seen several new developments. In this survey, we study today`s most prominent Meaning Representation Frameworks. We shed light on their theoretical properties, as well as on their practical research environment, i.e., on datasets, parsers, applications, and future challenges."
}



@article{watson,
	title = {Building {Watson}: {An} {Overview} of the {DeepQA} {Project}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2371-9621},
	shorttitle = {Building {Watson}},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2303},
	doi = {10.1609/aimag.v31i3.2303},
	abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.},
	language = {en},
	number = {3},
	urldate = {2025-01-18},
	journal = {AI Magazine},
	author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya A. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
	month = jul,
	year = {2010},
	note = {Number: 3},
	pages = {59--79},
	file = {Full Text PDF:/home/rambip/Documents/Zotero/storage/UEQIXJFY/Ferrucci et al. - 2010 - Building Watson An Overview of the DeepQA Project.pdf:application/pdf},
}

@inproceedings{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@article{catala,
  title={Catala: a programming language for the law},
  author={Denis Merigoux and Nicolas Chataing and Jonathan Protzenko},
  journal={Proceedings of the ACM on Programming Languages},
  year={2021},
  volume={5},
  pages={1 - 29},
  url={https://api.semanticscholar.org/CorpusID:232110399}
}

@article{inadequacy_rule_based,
  title={Legal Expert Systems: The Inadequacy of a Rule-Based Approach},
  author={James Popple},
  journal={LSN: Legal Information Scholarship (Topic)},
  year={1990},
  url={https://api.semanticscholar.org/CorpusID:14767786}
}

@article{llm_for_legal_experts,
  title={From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems},
  author={Samyar Janatian and Hannes Westermann and Jinzhe Tan and Jarom{\'i}r {\vS}avelka and Karim Benyekhlef},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.04911},
  url={https://api.semanticscholar.org/CorpusID:265067125}
}
@inproceedings{expert_system_law,
  title={Expert systems in law},
  author={Antonio A. Martino},
  year={1992},
  url={https://api.semanticscholar.org/CorpusID:60429180}
}
@misc{legifrance,
  title = {{Legifrance} Website},
  howpublished = {\url{https://www.legifrance.gouv.fr/}},
  note = {Accessed: 2025-01-18}
}
@article{word_representation,
author = {Naseem, Usman and Razzak, Imran and Khan, Shah Khalid and Prasad, Mukesh},
title = {A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3434237},
doi = {10.1145/3434237},
abstract = {Word representation has always been an important research area in the history of natural language processing (NLP). Understanding such complex text data is imperative, given that it is rich in information and can be used widely across various applications. In this survey, we explore different word representation models and its power of expression, from the classical to modern-day state-of-the-art word representation language models (LMS). We describe a variety of text representation methods, and model designs have blossomed in the context of NLP, including SOTA LMs. These models can transform large volumes of text into effective vector representations capturing the same semantic information. Further, such representations can be utilized by various machine learning (ML) algorithms for a variety of NLP-related tasks. In the end, this survey briefly discusses the commonly used ML- and DL-based classifiers, evaluation metrics, and the applications of these word embeddings in different NLP tasks.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {74},
numpages = {35},
keywords = {language models, word representation, natural language processing, Text mining}
}
@article{ontology_induction_survey,
author = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
year = {2011},
month = {01},
pages = {1-36},
title = {Ontology Learning from Text: A Look Back and into the Future},
volume = {44},
journal = {ACM Computing Surveys - CSUR},
doi = {10.1145/2333112.2333115}
}

@inproceedings{ontology_alignment_legal,
  title={Ontology Population and Alignment for the Legal Domain: YAGO, Wikipedia and LKIF},
  author={Cristian Cardellino and Milagro Teruel and Laura Alonso Alemany and Serena Villata},
  booktitle={International Workshop on the Semantic Web},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:4543463}
}

@misc{legal_question_rag,
      title={Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models}, 
      author={Antoine Louis and Gijs van Dijck and Gerasimos Spanakis},
      year={2023},
      eprint={2309.17050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17050}, 
}

@inproceedings{similarity_prediction,
    title = "Complex Labelling and Similarity Prediction in Legal Texts: Automatic Analysis of {F}rance`s Court of Cassation Rulings",
    author = "Charmet, Thibault  and
      Cherichi, In{\`e}s  and
      Allain, Matthieu  and
      Czerwinska, Urszula  and
      Fouret, Amaury  and
      Sagot, Beno{\^i}t  and
      Bawden, Rachel",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.509/",
    pages = "4754--4766",
    abstract = "Detecting divergences in the applications of the law (where the same legal text is applied differently by two rulings) is an important task. It is the mission of the French Cour de Cassation. The first step in the detection of divergences is to detect similar cases, which is currently done manually by experts. They rely on summarised versions of the rulings (syntheses and keyword sequences), which are currently produced manually and are not available for all rulings. There is also a high degree of variability in the keyword choices and the level of granularity used. In this article, we therefore aim to provide automatic tools to facilitate the search for similar rulings. We do this by (i) providing automatic keyword sequence generation models, which can be used to improve the coverage of the analysis, and (ii) providing measures of similarity based on the available texts and augmented with predicted keyword sequences. Our experiments show that the predictions improve correlations of automatically obtained similarities against our specially colelcted human judgments of similarity."
}

@misc{few_shot_learners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@inproceedings{legalbench,
 author = {Guha, Neel and Nyarko, Julian and Ho, Daniel and R\'{e}, Christopher and Chilton, Adam and K, Aditya and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and Talisman, Dmitry and Hoque, Enam and Surani, Faiz and Fagan, Frank and Sarfaty, Galit and Dickinson, Gregory and Porat, Haggai and Hegland, Jason and Wu, Jessica and Nudell, Joe and Niklaus, Joel and Nay, John and Choi, Jonathan and Tobia, Kevin and Hagan, Margaret and Ma, Megan and Livermore, Michael and Rasumov-Rahe, Nikon and Holzenberger, Nils and Kolt, Noam and Henderson, Peter and Rehaag, Sean and Goel, Sharad and Gao, Shang and Williams, Spencer and Gandhi, Sunny and Zur, Tom and Iyer, Varun and Li, Zehua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {44123--44279},
 publisher = {Curran Associates, Inc.},
 title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/89e44582fd28ddfea1ea4dcb0ebbf4b0-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}
